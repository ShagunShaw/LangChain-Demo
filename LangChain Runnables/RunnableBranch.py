# It is the If-Else part of a Runnable Flow

from langchain_openai import ChatOpenAI
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from dotenv import load_dotenv

from langchain.schema.runnable import RunnableSequence, RunnableLambda, RunnablePassthrough, RunnableParallel, RunnableBranch

load_dotenv()

prompt1= PromptTemplate(
    template= "Write a detailed report on {topic}" , 
    input_variables= ["topic"]
)

prompt2= PromptTemplate(
    template= "Summarize the following text: {text}" , 
    input_variables= ["text"]
)

model = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0)

parser= StrOutputParser()

report_gen_chain = RunnableSequence(prompt1, model, parser)

# The basic idea of this application is that if the report generated by the LLM is less than 5000, then send it to the user as it is, but if it is more than 5000 characters, then summarize it first and then send it to the user. 
branch_chain= RunnableBranch(
    (lambda x: len(x) > 5000,   RunnableSequence(prompt2, model, parser)),  
    default= RunnablePassthrough()
)

final_chain= RunnableSequence(
    report_gen_chain,
    branch_chain
)

result= final_chain.invoke({"topic": "Artificial Intelligence"})

print(result)